# Github Discussion: "Diarization pipeline output variability with different CUDA versions.  #1370"

## Source: https://github.com/pyannote/pyannote-audio/issues/1370#top

## Reply
popcornell
opened on May 12, 2023

Hi,

I just want to report I observed quite a large variability in the output of the pyannote diarization pipeline if I change the CUDA version on my side.
For example, switching between torch 1.13.1-py3.9_cuda11.7_cudnn8.5.0_0 --> 1.13.1-py3.9_cuda11.6_cudnn8.3.2_0 by running:

    conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia
    conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia

In my case quite changes the output of the pipeline.
For example a simple script such as (I use an audio from CHiME-6 but with other long audio files I observed it as well):

from pyannote.audio import Model, Pipeline
import torch
import soundfile as sf


torch.manual_seed(0)
audio, fs = sf.read("/raid/users/popcornell/chime7espnet/egs2/chime7_task1/diar_asr1/chime7_task1/chime6/audio/dev/S02_U06.CH1.wav")
audio = torch.from_numpy(audio).float()
pipeline = Pipeline.from_pretrained(
        "pyannote/speaker-diarization",
        use_auth_token="PUT_YOUR_TOKEN")
#pretrained_pipeline.cuda()
out_seg = pipeline({"waveform": audio[None, ...], "sample_rate": fs})

with open("predictions.rttm", "w") as f:
    f.write(out_seg.to_rttm())

I get slighly different predictions.rttm outputs when switching the CUDA version.
The relative DER (no collar) of the two predictions is 12% and is a lot.
On the CHiME-7 challenge diarization baseline https://github.com/espnet/espnet/tree/master/egs2/chime7_task1/diar_asr1 I observe the same and is leading to difficulties in replicating the diarization baseline unfortunately (up to 8% DER difference has been observed between different participants).

The two RTTMs are attached here as .txt files as GitHub does not allow .rttm (116 for 11.6 CUDA and 117 for 11.7).
predictions_116.txt
predictions_117.txt

I also attach info about my conda environment here.
conda_env_info.txt
as well as the explicit conda config file to reproduce the environment conda install --name myenv --file cornell_env.txt 
cornell_env.txt

Info about my system too by running:

wget https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py
python collect_env.py

is attached:
output_collect_env.txt

I think it is coming from the embedding extraction and clustering step and not from segmentation because the segmentation seems consistent till the second decimal digit when swithcing CUDA
Activity
github-actions
github-actions commented on May 12, 2023
github-actions
on May 12, 2023

Thank you for your issue. Give us a little time to review it.

PS. You might want to check the FAQ if you haven't done so already.

This is an automated reply, generated by FAQtory
hbredin
hbredin commented on May 12, 2023
hbredin
on May 12, 2023
Member

Thanks @popcornell for the detailed bug report.

Can you please share the artifacts.pkl files resulting from this sample piece of code (for at least 2 different CUDA configurations) ?

This should help us pinpoint where the problem is coming from.

import pickle
from pyannote.audio.pipelines.utils.hook import logging_hook
file = {"audio": "/path/to/audio.wav"}
file["diarization"] = pipeline(file, hook=hook)
with open("artifacts.pkl", "wb") as f:
    pickle.dump(file, f)

popcornell
popcornell commented on May 20, 2023
popcornell
on May 20, 2023
Author

Hi Hervé, sorry for the late update.

Here are the files for 11.6 CUDA and 11.7 CUDA.
I had to use gdrive because files over 25 MB are not allowed on GitHub.

https://drive.google.com/file/d/1sEimBFylumaXeeSSWaZa_TjMVBKAUTSD/view?usp=share_link
hbredin
hbredin commented on May 21, 2023
hbredin
on May 21, 2023
Member

Hmmmm. Did I miss something? The diarization outputs seem identical...
popcornell
popcornell commented on May 22, 2023
popcornell
on May 22, 2023 · edited by popcornell
Author

My bad, the CUDA version did not change for some reason between the two runs.
I have updated the files and should be correct now

>>> der = DiarizationErrorRate()
>>> der(b["diarization"], a["diarization"])
/raid/users/popcornell/CHiME6/test_chime7/espnet/tools/venv/lib/python3.9/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.
  warnings.warn(
0.12118828433555145

the difference seems 12% DER on my side.
hbredin
hbredin commented on May 22, 2023
hbredin
on May 22, 2023
Member

Thanks. There is indeed huge difference, even for the segmentation step.
Could you also share the pickle when run on CPU?
hbredin
hbredin commented on May 22, 2023
hbredin
on May 22, 2023
Member

Possibly related: https://discuss.pytorch.org/t/cuda-11-7-causing-large-calculation-discrepancy/171344/2
hbredin
hbredin commented on May 22, 2023
hbredin
on May 22, 2023
Member

Did you run this on Ampere devices?

https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
popcornell
popcornell commented on May 22, 2023
popcornell
on May 22, 2023
Author

Yes A100 on a DGXA100 system.

My bet is that this is something on Pytorch side rather than Pyannote anyway. Just raised this issue so we can confirm or not.

If it is Pytorch issue, such huge difference (despite even the fact the code is open-source) is kinda of a major scientific problem.
Considering I have made effort in trying to make it replicable by open-sourcing the code you can imagine how issues like these cast doubts on most of the works out there (as most do not even share the code either).
hbredin
hbredin commented on May 22, 2023
hbredin
on May 22, 2023
Member

Even so, could you try them both with the following?

torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False

popcornell
popcornell commented on May 22, 2023
popcornell
on May 22, 2023
Author

It seems the results are now consistent !
hbredin
mentioned this on May 23, 2023

VAD Significant inconsistency in the results when using CPU and GPU #1013

    fix(pipeline): fix reproducibility issue with Ampere CUDA device #1381

hbredin
hbredin commented on May 23, 2023
hbredin
on May 23, 2023 · edited by hbredin
Member

Great! Would this "fix" be a good solution?
popcornell
popcornell commented on May 23, 2023
popcornell
on May 23, 2023
Author

Yeah it looks good